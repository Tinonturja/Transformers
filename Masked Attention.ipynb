{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "238af29d-b936-41e0-98dd-d8b81e120ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac24fe6b-5fb1-44fc-ac85-e2906ade31d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self, d_model = 2,\n",
    "                 row_dim = 0,\n",
    "                 col_dim = 1):\n",
    "        super().__init__()\n",
    "        #initialize random query, key and value weights and linear function also do the matrix multiplication\n",
    "        self.W_q = nn.Linear(in_features = d_model, \n",
    "                             out_features = d_model,\n",
    "                             bias = False)\n",
    "        self.W_k = nn.Linear(in_features = d_model,\n",
    "                             out_features = d_model,\n",
    "                             bias = False)\n",
    "        self.W_v = nn.Linear(in_features = d_model,\n",
    "                             out_features = d_model,\n",
    "                             bias = False)\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "\n",
    "    def forward(self, tokenencoddings,mask = None):\n",
    "        q = self.W_q(tokenencoddings)\n",
    "        k = self.W_k(tokenencoddings)\n",
    "        v = self.W_v(tokenencoddings)\n",
    "        sims = torch.matmul(q,k.transpose(self.row_dim,self.col_dim))\n",
    "        scaled_sims = sims/ torch.tensor(k.size(self.col_dim)**0.5)\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask = mask, value = -1e9)\n",
    "        Attention_percent = F.softmax(scaled_sims, dim = self.col_dim)\n",
    "\n",
    "        Attention_scores = torch.matmul(Attention_percent,v)\n",
    "\n",
    "        return(Attention_scores)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43de9f1e-ccb0-4b05-9bae-d92967846b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create a matrix of token encodings...\n",
    "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
    "                                 [0.57, 1.36],\n",
    "                                 [4.41, -2.16]])\n",
    "\n",
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create a masked self-attention object\n",
    "maskedSelfAttention = MaskedAttention(d_model=2,\n",
    "                               row_dim=0,\n",
    "                               col_dim=1)\n",
    "\n",
    "## create the mask so that we don't use\n",
    "## tokens that come after a token of interest\n",
    "mask = torch.tril(torch.ones(3, 3))\n",
    "mask = mask == 0\n",
    "mask # print out the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad39cb68-55f1-4310-9e06-0e9a4cfcadbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6038,  0.7434],\n",
       "        [-0.0062,  0.6072],\n",
       "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate masked self-attention\n",
    "maskedSelfAttention(encodings_matrix, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22647846-5520-4d82-9181-a204fbbcb889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit matrix: \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      " lower triangular matrix: \n",
      " tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "Representation with true and false \n",
      "tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "#creating mask\n",
    "m = torch.ones(3,3)\n",
    "print(f\"Unit matrix: \\n{m}\")\n",
    "\n",
    "n = torch.tril(m)\n",
    "print(f\" lower triangular matrix: \\n {n}\")\n",
    "\n",
    "mask = n==0\n",
    "print(f\"Representation with true and false \\n{mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc7108f7-ea67-4059-b37f-f46f4c089dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function tensor in module torch:\n",
      "\n",
      "tensor(...)\n",
      "    tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) -> Tensor\n",
      "    \n",
      "    Constructs a tensor with no autograd history (also known as a \"leaf tensor\", see :doc:`/notes/autograd`) by copying :attr:`data`.\n",
      "    \n",
      "    .. warning::\n",
      "    \n",
      "        When working with tensors prefer using :func:`torch.Tensor.clone`,\n",
      "        :func:`torch.Tensor.detach`, and :func:`torch.Tensor.requires_grad_` for\n",
      "        readability. Letting `t` be a tensor, ``torch.tensor(t)`` is equivalent to\n",
      "        ``t.clone().detach()``, and ``torch.tensor(t, requires_grad=True)``\n",
      "        is equivalent to ``t.clone().detach().requires_grad_(True)``.\n",
      "    \n",
      "    .. seealso::\n",
      "    \n",
      "        :func:`torch.as_tensor` preserves autograd history and avoids copies where possible.\n",
      "        :func:`torch.from_numpy` creates a tensor that shares storage with a NumPy array.\n",
      "    \n",
      "    Args:\n",
      "        data (array_like): Initial data for the tensor. Can be a list, tuple,\n",
      "            NumPy ``ndarray``, scalar, and other types.\n",
      "    \n",
      "    Keyword args:\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, infers data type from :attr:`data`.\n",
      "        device (:class:`torch.device`, optional): the device of the constructed tensor. If None and data is a tensor\n",
      "            then the device of data is used. If None and data is not a tensor then\n",
      "            the result tensor is constructed on the current device.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "        pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      "            the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      "    \n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
      "        tensor([[ 0.1000,  1.2000],\n",
      "                [ 2.2000,  3.1000],\n",
      "                [ 4.9000,  5.2000]])\n",
      "    \n",
      "        >>> torch.tensor([0, 1])  # Type inference on data\n",
      "        tensor([ 0,  1])\n",
      "    \n",
      "        >>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n",
      "        ...              dtype=torch.float64,\n",
      "        ...              device=torch.device('cuda:0'))  # creates a double tensor on a CUDA device\n",
      "        tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n",
      "    \n",
      "        >>> torch.tensor(3.14159)  # Create a zero-dimensional (scalar) tensor\n",
      "        tensor(3.1416)\n",
      "    \n",
      "        >>> torch.tensor([])  # Create an empty tensor (of size (0,))\n",
      "        tensor([])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3991b2e-7232-4b25-9767-93087802e1a2",
   "metadata": {},
   "source": [
    "#### Some Basic Assesment of Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb53f76-52c4-4dd6-a5d7-136bfc5f98ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\n",
    "print(a)\n",
    "\n",
    "b = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(b)\n",
    "\n",
    "print(a.size())\n",
    "\n",
    "\n",
    "\n",
    "a.size(2)\n",
    "\n",
    "torch.rand(2,2)\n",
    "\n",
    "o = torch.ones(3,3)\n",
    "o\n",
    "\n",
    "f = torch.tril(o)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380cb4e-d021-42cd-8ebb-148e494a8839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
