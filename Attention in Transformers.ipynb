{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a981ec-3f21-4254-925b-2e6c2075a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## torch let's us create a tensor\n",
    "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
    "import torch.nn.functional as f ## that gives us the softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eba41c-0516-4086-818f-0adf694809bf",
   "metadata": {},
   "source": [
    "#### nn.Module is the base class for all neural network modules that you make with pytorch\n",
    "#### d_module is the dimension of the model or the number of word embeddings values per token, we will d_model to define the size of all the weight matrices that we'll use to create the queries, keys and values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b9149b4-cbda-4667-ad7d-5b454ee6ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model=2,\n",
    "                 row_dim = 0,\n",
    "                 col_dim = 1):\n",
    "        #d_model = the number of embedding model values per token. token is the word repository basically or corpus in other way\n",
    "        ## row_dim , col_dim = the indices we should use to access rows or columns\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(in_features = d_model, #self.W_q creates the untrained query matrix and the linear function not just store the value but also do the math when the time comes\n",
    "                             out_features = d_model,\n",
    "                             bias = False) #in features = row of the query weight matrix\n",
    "                                           ## out_features = column of the query weight matrix\n",
    "        self.W_k = nn.Linear(in_features = d_model,\n",
    "                             out_features = d_model, #shape (2 x 2 Matrix)\n",
    "                             bias = False)\n",
    "        self.W_v = nn.Linear(in_features = d_model,\n",
    "                             out_features = d_model,\n",
    "                             bias = False)\n",
    "        self.row_dim= row_dim\n",
    "        self.col_dim = col_dim\n",
    "        #linear function do the matrix multiplication too\n",
    "    def forward(self, token_encodings): #this mehtod actually calculated the self attention\n",
    "        q = self.W_q(token_encodings) ##shape 3x2 matrix multiply with 2x2 matrix,\n",
    "        k = self.W_k(token_encodings) ##output size will be 3x2 matrix\n",
    "        v = self.W_v(token_encodings)\n",
    "\n",
    "\n",
    "        sims = torch.matmul(q,k.transpose(dim0 = self.row_dim, dim1 = self.col_dim)) #3x2 matrix will be multiplied by 2x3 matrix\n",
    "        scaled_sims = sims/torch.tensor(k.size(self.col_dim)**0.5)\n",
    "        attention_percents = f.softmax(scaled_sims, dim = self.col_dim)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        return attention_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1fdc55-007d-492a-884e-3ff4476e8991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings_matrix = torch.tensor([[1.16,0.23],\n",
    "                                 [0.57,1.36],\n",
    "                                 [4.41,-2.16]])\n",
    "torch.manual_seed(42)\n",
    "selfAttention = SelfAttention(d_model = 2,\n",
    "                              row_dim=0,\n",
    "                              col_dim =1)\n",
    "selfAttention(encodings_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cf91ad8-8d3d-4c59-95ce-b9e072e9a538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings_matrix.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d943585-5f24-4fed-a34d-670d30c6b9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings_matrix.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "def82ad0-e28b-4abb-ac9a-d5e63beaa568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1549, -0.3443],\n",
       "        [ 0.1427,  0.4153]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#untrained key weights\n",
    "selfAttention.W_k.weight.transpose(0,1) #why do we use (0,1) here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45fbe57a-01e0-4684-a77b-3ac646bb75e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1549, -0.3443],\n",
       "        [ 0.1427,  0.4153]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfAttention.W_k.weight.transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a3ec326-d06d-430e-96d8-d81ae2fb53a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1469, -0.3038],\n",
       "        [ 0.1057,  0.3685],\n",
       "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trained key values\n",
    "selfAttention.W_k(encodings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "317e8bc5-6ab5-4cb2-811e-babf0363a512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1469, -0.3038],\n",
       "        [ 0.1057,  0.3685],\n",
       "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q= torch.matmul(encodings_matrix,selfAttention.W_k.weight.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae0e3f4c-f333-43a4-9cea-6e45070cff9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1549,  0.1427],\n",
       "        [-0.3443,  0.4153]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfAttention.W_k.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a60e8-cf63-401f-9d34-7eba17c75179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
